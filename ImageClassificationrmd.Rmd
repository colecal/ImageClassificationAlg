---
title: "Project3_rmd"
author: "Cole Calderon"
date: '2023-07-25'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r setup2, include=FALSE}
#Deep Learning Image Classifier Project

#Cole Calderon

##Import Libraries and Dataset
library(keras)
install_keras()
library(tensorflow)
install_tensorflow()

library(ggplot2)
library(tidyr)
f_m <- dataset_fashion_mnist()
```

```{r traintest}
##seperate into train and test data
c(train_images, train_labels) %<-% f_m$train
c(test_images, test_labels) %<-% f_m$test

class_names = c('T-shirt/top',
                'Trouser',
                'Pullover',
                'Dress',
                'Coat',
                'Sandal',
                'Shirt',
                'Sneaker',
                'Bag',
                'Ankle boot')
##Data Spelunking
dim(train_images)
dim(train_labels)

train_labels[1:10]

dim(test_images)
dim(test_labels)
```

```{r plot1, echo=FALSE}
##inspect single image
i_1 <- as.data.frame(train_images[1, , ])
colnames(i_1) <- seq_len(ncol(i_1))
i_1$y <- seq_len(nrow(i_1))
i_1 <- gather(i_1, "x", "value", -y)
i_1$x <- as.integer(i_1$x)


ggplot(i_1, aes(x = x, y = y, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "black", na.value = NA) +
  scale_y_reverse() +
  theme_minimal() +
  theme(panel.grid = element_blank()) + 
  theme(aspect.ratio = 1) + 
  ylab("") +
  xlab("")
```

```{r plot1_normalize, echo=FALSE}
## Scale the train and test data
train_images <- train_images / 255
test_images <- test_images / 255
###This will normalize the data prior to feeding NNW
##inspect single image
i_1 <- as.data.frame(train_images[1, , ])
colnames(i_1) <- seq_len(ncol(i_1))
i_1$y <- seq_len(nrow(i_1))
i_1 <- gather(i_1, "x", "value", -y)
i_1$x <- as.integer(i_1$x)

ggplot(i_1, aes(x = x, y = y, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "black", na.value = NA) +
  scale_y_reverse() +
  theme_minimal() +
  theme(panel.grid = element_blank()) + 
  theme(aspect.ratio = 1) + 
  ylab("") +
  xlab("")
###Each image is greyscale
```

```{r plot2, echo=FALSE}
##plot each different clothing item
par(mfcol=c(5,5))
par(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')
for (i in 1:25) { 
  img <- train_images[i, , ]
  img <- t(apply(img, 2, rev)) 
  image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',
        main = paste(class_names[train_labels[i] + 1]))
}
```

```{r model1, echo=FALSE, warning=FALSE}
##Building the NNW
model <- keras_model_sequential()
model %>%
  layer_flatten(input_shape = c(28, 28)) %>%
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax')
summary(model)

#dense or fully connected nnw layers
#128 layers
```

```{r model2, echo=FALSE, warning=FALSE}
##Compiling the model
model %>% compile(
  optimizer = 'adam', 
  loss = 'sparse_categorical_crossentropy',
  metrics = c('accuracy')
)
summary(model)
```

```{r model3, echo=FALSE, warning=FALSE}
##Training and Testing the model
model %>% fit(
  train_images, train_labels,
  epochs = 10, validation_split=0.2)

score <- model %>% evaluate(test_images, test_labels)

score

#The accuracy is fairly high which suggests this is solid model to go ahead with. However, increasing number of epochs could positively influence the model.
```

![ten_epoch](%5C10epochs.png)

```{r model4, echo=FALSE, warning=FALSE}
##Training and Testing the model
model %>% fit(
  train_images, train_labels,
  epochs = 25, validation_split=0.2)

score <- model %>% evaluate(test_images, test_labels)

score

#Even though the accuracy is higher, this does not immediately suggest that it is a better model since we could be over fitting. However, we can see the validation accuracy is also fairly high but does not increase as significantly over epochs as the training accuracy does.

#Furthermore, the loss value increases significantly more than the accuracy does over this many epochs which suggests multiple epochs is not neccesarily needed too make a better model.
```

![25epochs](%5C25epochs.png)

```{r output1, echo=FALSE, warning=FALSE}
##Making Predictions on Test Data

predictions <- model %>% predict(test_images)
predictions[1, ]

which.max(predictions[1, ])
class_pred <- model %>% predict(test_images)
class_pred[1:20]
test_labels[1:20]

```

```{r output2, echo=FALSE, warning=FALSE}
##plotting with predictions

par(mfcol=c(5,5))
par(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')
for (i in 1:25) { 
  img <- test_images[i, , ]
  img <- t(apply(img, 2, rev)) 
  # subtract 1 as labels go from 0 to 9
  predicted_label <- which.max(predictions[i, ]) - 1
  true_label <- test_labels[i]
  if (predicted_label == true_label) {
    color <- '#008800' 
  } else {
    color <- '#bb0000'
  }
  image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',
        main = paste0(class_names[predicted_label + 1], " (",
                      class_names[true_label + 1], ")"),
        col.main = color)}

```
